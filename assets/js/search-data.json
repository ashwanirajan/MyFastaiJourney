{
  
    
        "post0": {
            "title": "Title",
            "content": "Starts with a basic introduction of terms. I don&#39;t need to type what classification and regression is at this stage of my career. :D . Learner(data, architecture, metric): it contains labelled data and the model. So learner tried to find what are the best prameters for the model which fit the data and labels gien to it. Metric is a function called out on validation set after each epoch that predicts the quality of prediction of our architecture. Epoch is when the entire training set is covered once. . Loss vs Metric: Metrics normally depend on the accuracy of predictions, i.e. as long as 640/1000 examples are classified correctly, the metric would remain same. Metrics are generally not very sensitive of the small changes in weights/parameters, e.g. Error rate, accuracy etc. On the other hand, loss changes even on changing the parameters by very small extent. . Metric is the ting we care about, loss is what the computer is using to update the parameters to improve the model performance. . Overfitting: Memorizing a particultar dataset on which model is trained and not learning about the classification task for which the model is trained, i.e. identifying cat/dog breeds in this case. . learn.fine_tune(): used for transfer learning. Weights of pre-trained models are updated using a different dataset(our dataset) that what the model was originally trained on for additional epochs. one epoch to just fit the parts of model which have been added to the model for this particular dataset, then as many additional epochs as required to fit the entire model, especially the earlier layers which need to change slower and have been observed that they don&#39;t change much. . Catastrophic Forgetting A model pe-trained on image-net would forget the information it learned from imagenet once it is fine tuned for our purpose. To keep it good at both tasks, we need to put examples from previous tasks as well. . Visualising Alexnet: layer by layer visualisation of CNN filters and the part of images which activate those filters. As we move deeper, we see that the model is able to identify more and more sophisticated elements. . CNNs are used to identify other things as well, which can be converted to pictures. E.g. sound by plotting their frequency over time, fraud detection by using mouse clicks and tracings etc. . Find Different state of the art pre-trained models for transfer learning at ModelZoo . Deep learning not always good choice in tabular data but works well at data involving high cardinal variables like zip code, product id etc. . The Practice of Deep Learning . Vision: Detection, Classification | Text: Classification, Conversation(pretty bad at it) | Tabular: High Cardinality | RecSys: Predication &lt;&gt; Recommendation | Multimodal: different types of data, eg: putting captions on photos, | Other: ULNfit(for NLP) is good at protein analysis. | Recommendations vs Predictions: What Amazon does is that it shows you the prediction result of their collaborative filtering model, ie if someone has bought a bok from an author, Amazon tries to advertise the same author&#39;s books instead of the same genre/type of book that i am interested in. If I am interested in an author&#39;s book,I would probably know about the other books of the same author, an better way to recommend would be to give you books of same genre/kind. . Some Stuff about P-values and the practical implementation of a Deep learning Model . Null Hypothesis Significance Testing Never Worked - Prof. Frank Harrell, Vanderbilt University, Prof. of Biostatistics. . P-values also disapproved by American Statistical Association: they released a press note of 7 points about p-values: It then provides six principles to guide p-value usage: . https://www.crossfit.com/health/the-asa-statement-on-p-values-context-process-and-purpose#:~:text=The%20ASA%20defines%20the%20p,extreme%20than%20its%20observed%20value. . P-values indicate how incompatible the data are with a specified statistical model. In the case of “null hypothesis” testing, a lower p-value indicates the belief that there is no difference between two tested groups (e.g., a control and a treatment group). The lower the p-value, the less compatible the data are with the null hypothesis. | P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. The p-value merely tells us how compatible a particular explanation of the data (i.e., that there is no difference between groups) is with the data itself. It does not tell us how likely this explanation, or its converse, is to be true. | Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold. The design of the study, the quality of the measurements, and other data surrounding the studied phenomenon should all inform how a p-value is interpreted. A single p-value cannot denote the truth or falsehood of a statement or hypothesis. The widespread use of “statistical significance” (generally interpreted as “p 0.05”) as a license for making a claim of a scientific finding (or implied truth) leads to considerable distortion of the scientific process. | Proper inference requires full reporting and transparency. Selectively reporting p-values, calculating p-values only for some hypotheses, or discussing only data where the calculated p-value was below a specific threshold renders the data distorted and uninterpretable. The p-value of a particular analysis can only be assessed with complete transparency regarding other analyses performed and their results. | A p-value, or statistical significance, does not measure the size of an effect or the importance of a result. A very low p-value may indicate a small or unimportant event measured very precisely, while a large or important effect may generate a large p-value if measurement precision is low. The p-value tells us nothing about the size of an effect or its scientific, human, or economic significance. | By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis. A low p-value provides only weak evidence against a null hypothesis, while a high p-value does not imply the truth of the null. Other hypotheses may be more consistent with the observed data. Any p-value should be the start, not end, of an analytic approach. | Covid-19 paper . The covid-19 paper has done multiple linear regression and listed the variables which show a relationship, i.e. a p-value &lt; 0.01 with three stars. . Important thing to note: if p-value &gt; threshold(0.01/0.05/0.1 etc) then it does not mean that no relationship b/w two variables exist(or the null hypothesis is true/such a distribution is random). There could still be a relationship, only that there could be a few more dependent variables playing a role and leading to a random distribution when we look at the relationship b/w two variables. . Another example is when we don&#39;t have enough data. Suppose there are only 10 data points, we might not get a p-value &lt;0.01. That doesn&#39;t mean there is no relationship. The way to make sure is that, we try the exact opposite null hypothesis and check if we can reject that null hypothesis. If not, then it means that we just don&#39;t have enough data to conclude anything. . The paper actually does a multivariate regression model with temperature, humidity, population density and GDP, but the graph only shows a univariate relationship. So the variation in R vs Temp plot is not randon, there are a lot of other control variables like Pop. density and others mentioned above which result in these variations. . Jeremy says, p-value is not a mesure of the practical importance, instead the slope is. The Equation R = A - BT - CRel. Humidity, is obtained when control variables are omitted and this suggests that if this model is correct it shows that 2 cities which are exactly same in terms of other variables but have different T and Humidity might have very different spread. . Starting Your Project . Designing Great Data products: https://www.oreilly.com/radar/drivetrain-approach-data-products/ . Need to revisit: All 4 possibilities and their consequences. The concept of prior. . Gathering Data - Bing API Search . To download images with Bing Image Search, sign up at Microsoft for a free account. You will be given a key, which you can copy and enter in a cell as follows (replacing &#39;XXX&#39; with your key and executing it): . key = &#39;27109ffc3c0649ef853678985f164701&#39; . got the key here: https://azure.microsoft.com/en-us/try/cognitive-services/my-apis/?apiSlug=search-api-v7 . from azure.cognitiveservices.search.imagesearch import ImageSearchClient as api from msrest.authentication import CognitiveServicesCredentials as auth . I had made an azure account with a previous microsoft account(from IITG) that I had. It was quite different from what is mentioned in the forum post mentioned by Jeremy. I had three options while making the account, 1. get a 7 day free trial, 2. Get free Azure account and 3. Get paid subscription. I went for 2. get free azure account. It was repeatedly mentioned that I won&#39;t be charged until I upgrade, but when I went to the API page, I see that I have only a 7 day free trial available. I had to give in my Card details though. . def search_images_bing(key, term, min_sz = 128): client = api(&#39;https://api.cognitive.microsoft.com&#39;, auth(key)) return L(client.images.search(query=term,count = 150, min_height = min_sz, min_width = min_sz).value) . L is a class in fastbook. It is fastai&#39;s alternatiive for python lists. . search_images_bing . &lt;function __main__.search_images_bing(key, term, min_sz=128)&gt; . api actually returns 150 items. There are not links. Each item is a dictionary with some keys. One key is &#39;content_url&#39; which is the only thing required for us now. So the L class has an attribute &#39;attrgot&#39; which can be used to get the value of a particular key name from all idems, This is done in the next cell. . results = search_images_bing(key, &#39;soccer ball&#39;) ims = results.attrgot(&#39;content_url&#39;) len(ims) . 150 . Looking at Some Examples . # print(i) . So now ims has a list of URLs for soccer ball images. You can download content from a URL at a particular destination like given in the below slide. . dest = &#39;images/soccerball1.jpg&#39; download_url(ims[0], dest) . Image.open() is from PIL library, which does not need to be called separately because it is one of the dependencies which is called when you call &#39;From fastbook import *&#39; . im = Image.open(dest) im.to_thumb(128,128) . Creating Data Folders . ball_types = &#39;cricket&#39;,&#39;soccer&#39;,&#39;baseball&#39; path = Path(&#39;balls&#39;) ## creates a path in current folder . if not path.exists(): path.mkdir() for o in ball_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} ball&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . fns = get_image_files(path) ## geting the folder path for all images in balls folder. ## Here all the image paths in all three folders are listed down. fns . (#443) [Path(&#39;balls/cricket/00000007.jpg&#39;),Path(&#39;balls/cricket/00000006.jpg&#39;),Path(&#39;balls/cricket/00000004.jpg&#39;),Path(&#39;balls/cricket/00000003.jpg&#39;),Path(&#39;balls/cricket/00000008.jpg&#39;),Path(&#39;balls/cricket/00000002.jpg&#39;),Path(&#39;balls/cricket/00000005.jpg&#39;),Path(&#39;balls/cricket/00000000.png&#39;),Path(&#39;balls/cricket/00000001.jpg&#39;),Path(&#39;balls/cricket/00000009.png&#39;)...] . Removing non-image paths . failed = verify_images(fns) ## checks whether that path contains image or not. failed . (#0) [] . path.unlink deletes files. https://stackoverflow.com/questions/42636018/python-difference-between-os-remove-and-os-unlink-and-which-one-to-use/42636082 . .map() calls the function inside the beackets on each element of the object it is attributed to. It is a part of L class. Here, map() calls path.unlink() function on each element of list L=failed. . failed.map(Path.unlink); . From Data to DataLoaders . ImageDataLoaders is a factory method. We have a more flexible way called DataBlock. . balls = DataBlock( blocks=(ImageBlock, CategoryBlock), #independent and dependent variable get_items=get_image_files, # get list of all filenames needed, See 3rd cell in create data folders section to know about function splitter=RandomSplitter(valid_pct=0.2, seed=42), #seed to fix the validation set everytime we run it get_y=parent_label, # how do we label the data.(By the name of parent folder, a pre-defined way in pytorch) item_tfms=Resize(128)) #resize images . The workflow of DataBlock: . get_items is called first. This would list out paths to our entire data. | get_x, get_y is called next. This would define the data and labels | blocks[0], blocks[1] is called. Need to read more on what these mean. | item_tfms are called to include all the required transformations of independent var. &gt; splitter is called which splits the data into training and validation . | A dataloader is called. Dataloader takes a batch(default 64) images so that this batch is run together on the GPU. . | batch_tfms: later | Datablocks return a dataloader a training and validation dataloaders. Dataloaders are a set of 64 images stacked together for one GPU run. . dls = balls.dataloaders(path) . path # the folder with data is given to dataloaders as input . Path(&#39;balls&#39;) . dls.valid.show_batch(max_n=4, nrows=1) # to see a batch from validation set . Training Your Model . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.218184 | 0.134097 | 0.034091 | 00:06 | . epoch train_loss valid_loss error_rate time . 0 | 0.313904 | 0.057729 | 0.011364 | 00:06 | . 1 | 0.240513 | 0.051974 | 0.011364 | 00:06 | . 2 | 0.175671 | 0.063505 | 0.011364 | 00:06 | . 3 | 0.138641 | 0.065315 | 0.011364 | 00:06 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(4, nrows=1) . Exporting Model to a .pkl File . learn.export() # exporting model in learn object as a pkl file . path1 = Path() # path to the current folder. Path() is used to define a path path1 . Path(&#39;.&#39;) . path1.ls(file_exts = &#39;.pkl&#39;) # list all files with .pkl extension . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path1/&#39;export.pkl&#39;) # load weights . learn_inf.predict(&#39;images/sachin_signed_ball.jpg&#39;) # this image is and out of sample image of cricket #ball signed by sachin. Our Model works!! . (&#39;cricket&#39;, tensor(1), tensor([0.0194, 0.7058, 0.2748])) .",
            "url": "https://ashwanirajan.github.io/MyFastaiJourney/2020/07/18/The-Sports-Ball-Classifier.html",
            "relUrl": "/2020/07/18/The-Sports-Ball-Classifier.html",
            "date": " • Jul 18, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ashwanirajan.github.io/MyFastaiJourney/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ashwanirajan.github.io/MyFastaiJourney/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}