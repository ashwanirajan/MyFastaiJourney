{
  
    
        "post0": {
            "title": "Here's the App",
            "content": "Gathering Data - Bing API Search . from fastbook import * from utils import * from fastai.vision.widgets import * . To download images with Bing Image Search, sign up at Microsoft for a free account. You will be given a key, which you can copy and enter in a cell as follows (replacing &#39;XXX&#39; with your key and executing it): . key = &#39;27109ffc3c0649ef853678985f164701&#39; . got the key here: https://azure.microsoft.com/en-us/try/cognitive-services/my-apis/?apiSlug=search-api-v7 . from azure.cognitiveservices.search.imagesearch import ImageSearchClient as api from msrest.authentication import CognitiveServicesCredentials as auth . I had made an azure account with a previous microsoft account(from IITG) that I had. It was quite different from what is mentioned in the forum post mentioned by Jeremy. I had three options while making the account, 1. get a 7 day free trial, 2. Get free Azure account and 3. Get paid subscription. I went for 2. get free azure account. It was repeatedly mentioned that I won&#39;t be charged until I upgrade, but when I went to the API page, I see that I have only a 7 day free trial available. I had to give in my Card details though. . def search_images_bing(key, term, min_sz = 128): client = api(&#39;https://api.cognitive.microsoft.com&#39;, auth(key)) return L(client.images.search(query=term,count = 150, min_height = min_sz, min_width = min_sz).value) . L is a class in fastbook. It is fastai&#39;s alternatiive for python lists. . search_images_bing . &lt;function __main__.search_images_bing(key, term, min_sz=128)&gt; . api actually returns 150 items. There are not links. Each item is a dictionary with some keys. One key is &#39;content_url&#39; which is the only thing required for us now. So the L class has an attribute &#39;attrgot&#39; which can be used to get the value of a particular key name from all idems, This is done in the next cell. . results = search_images_bing(key, &#39;soccer ball&#39;) ims = results.attrgot(&#39;content_url&#39;) len(ims) . 150 . Looking at Some Examples . # print(i) . So now ims has a list of URLs for soccer ball images. You can download content from a URL at a particular destination like given in the below slide. . dest = &#39;images/soccerball1.jpg&#39; download_url(ims[0], dest) . Image.open() is from PIL library, which does not need to be called separately because it is one of the dependencies which is called when you call &#39;From fastbook import *&#39; . im = Image.open(dest) im.to_thumb(128,128) . Creating Data Folders . ball_types = &#39;cricket&#39;,&#39;soccer&#39;,&#39;baseball&#39; path = Path(&#39;balls&#39;) ## creates a path in current folder . if not path.exists(): path.mkdir() for o in ball_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} ball&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . fns = get_image_files(path) ## geting the folder path for all images in balls folder. ## Here all the image paths in all three folders are listed down. fns . (#442) [Path(&#39;balls/baseball/00000000.jpg&#39;),Path(&#39;balls/baseball/00000001.png&#39;),Path(&#39;balls/baseball/00000002.jpg&#39;),Path(&#39;balls/baseball/00000003.png&#39;),Path(&#39;balls/baseball/00000004.png&#39;),Path(&#39;balls/baseball/00000005.jpg&#39;),Path(&#39;balls/baseball/00000006.jpg&#39;),Path(&#39;balls/baseball/00000007.jpg&#39;),Path(&#39;balls/baseball/00000008.jpg&#39;),Path(&#39;balls/baseball/00000009.jpg&#39;)...] . Removing non-image paths . failed = verify_images(fns) ## checks whether that path contains image or not. failed . (#0) [] . path.unlink deletes files. https://stackoverflow.com/questions/42636018/python-difference-between-os-remove-and-os-unlink-and-which-one-to-use/42636082 . .map() calls the function inside the beackets on each element of the object it is attributed to. It is a part of L class. Here, map() calls path.unlink() function on each element of list L=failed. . failed.map(Path.unlink); . From Data to DataLoaders . ImageDataLoaders is a factory method. We have a more flexible way called DataBlock. . balls = DataBlock( blocks=(ImageBlock, CategoryBlock), #independent and dependent variable get_items=get_image_files, # get list of all filenames needed, See 3rd cell in create data folders section to know about function splitter=RandomSplitter(valid_pct=0.2, seed=42), #seed to fix the validation set everytime we run it get_y=parent_label, # how do we label the data.(By the name of parent folder, a pre-defined way in pytorch) item_tfms=Resize(128)) #resize images . The workflow of DataBlock: . get_items is called first. This would list out paths to our entire data. | get_x, get_y is called next. This would define the data and labels | blocks[0], blocks[1] is called. Need to read more on what these mean. | item_tfms are called to include all the required transformations of independent var. &gt; splitter is called which splits the data into training and validation . | A dataloader is called. Dataloader takes a batch(default 64) images so that this batch is run together on the GPU. . | batch_tfms: later | Datablocks return a dataloader a training and validation dataloaders. Dataloaders are a set of 64 images stacked together for one GPU run. Dataloaders.train creates a training batch and dataloaders.valid creates a validation batch. Right now, it seems like a dataloader takes some images and does the augmentation and send it to model. . Data Augmentation . 1. Resizing . item_tfms = Resize(x), changes the size of images to x by x. How? Different Types of Resizes: . Squish: we squish any rectangle to size. Images would be different, bears might look fatter, balls could go oval. | Pad: we resize so that the shorter dimension is a match an use padding with pad_mode. Best method in terms of information retaining, but could lead to procssing of lot of zero-value pixels | Crop: we resize so that the larger dimension is match and crop (randomly on the training set, center crop for the validation set). We lose information | dls = balls.dataloaders(path) ## creating dataloaders. . path # the folder with data is given to dataloaders as input . Path(&#39;balls&#39;) . dls.valid.show_batch(max_n=4, nrows=1) # to see a batch from one validation dataloader . Datablock.new attribute: Create a new datablock which is an exact copy of the previous one, with some changes. I will use new() attribute to create a new datablock but with a different image transform. . balls = balls.new(item_tfms = Resize(100)) dls = balls.dataloaders(path) dls.valid.show_batch() . 2. RandomResizedCrop . min_scale = atlease this much percentage of pixels need to be involved. | unique = True, show_batch for same image. | On the validation set, we center crop the image if it&#39;s ratio isn&#39;t in the range (to the minmum or maximum value) then resize. . balls = balls.new(item_tfms = RandomResizedCrop(128, min_scale = 0.3)) dls = balls.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1, unique = True) . On the train set, random cropping takes place where scale in range (min_scale,1). . dls.train.show_batch(max_n=4, nrows=1, unique = True) . 3. Batch_tfms . item_tfms is done on each individual image and once we set all images to same size, batch transforms can be done. Few points about batch transforms: . done on an entire batch after loading that bach to GPU(loading using dataloaders) | for natural rgb images, there&#39;s a set of augmentations proved to work very well. These have been compiled in the aug_transforms() class. It provides a list of different transformations. | aug_transforms, by default are only applied to training dataloaders. | doc(aug_transforms) . aug_transforms(mult=1.0, do_flip=True, flip_vert=False, max_rotate=10.0, min_zoom=1.0, max_zoom=1.1, max_lighting=0.2, max_warp=0.2, p_affine=0.75, p_lighting=0.75, xtra_tfms=None, size=None, mode=&#39;bilinear&#39;, pad_mode=&#39;reflection&#39;, align_corners=True, batch=False, min_scale=1.0) . Random flip (or dihedral if flip_vert=True) with p=0.5 is added when do_flip=True. With p_affine we apply a random rotation of max_rotate degrees, a random zoom between min_zoom and max_zoom and a perspective warping of max_warp. With p_lighting we apply a change in brightness and contrast of max_lighting. Custon xtra_tfms can be added. size, mode and pad_mode will be used for the interpolation. max_rotate,max_lighting,max_warp are multiplied by mult so you can more easily increase or decrease augmentation with a single parameter. . p_affine, p_lighting, p could be percentage of images assigned to undergo each kind of augmentation . balls = balls.new(item_tfms = RandomResizedCrop(128, min_scale = 0.3)) ##, batch_tfms = aug_transforms(mult = 2)) can&#39;t run this due to memory error dls = balls.dataloaders(path) dls.train.show_batch( unique = True) #train dataloader is augmented. shft + tab in the #aug_transforms() bracket to see transforms applied. . dls.valid.show_batch( unique = True) ## no augmentations on valid dataloader . Training Your Model . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.588290 | 0.172980 | 0.056818 | 00:07 | . /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( . epoch train_loss valid_loss error_rate time . 0 | 0.436581 | 0.120037 | 0.068182 | 00:06 | . 1 | 0.298973 | 0.116487 | 0.034091 | 00:06 | . 2 | 0.220046 | 0.160748 | 0.045455 | 00:06 | . 3 | 0.180335 | 0.170473 | 0.045455 | 00:07 | . /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( . RuntimeError: DataLoader worker (pid 9945) is killed by signal: Killed. . If you&#39;re using the docker to run the PyTorch program, with high probability, it&#39;s because the shared memory of docker is NOT big enough for running your program in the specified batch size. . The solutions for this circumstance are: . use a smaller batch size to train your model. exit the current docker, and re-run the docker with specified &quot;--shm-size=16g&quot; or bigger shared memory space depending on your machine. Hope this could help those who have the same problem . . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( . interp.plot_top_losses(4, nrows=1) . Data Cleaning . Data cleaning normally happens before training. But lately, we can actually use the model outputs like plot_top_losses to do better data cleaning. I personally think that this User Itercative method of data cleaning is only useful in case of small datasets. It will show the images which lead to top losses and we can change the label/delete/keep those images. We normally prefer to train a short baseline model first and then clean the data by looking at the top losses . You can select top losses for validation and train set and decide what you want to clean and also which class you want to clean. From here, we can deal with the high loss data in training sample. Since all other data has lower losses, we can assume that they must be fine too. . doc(plot_top_losses) . cleaner = ImageClassifierCleaner(learn) cleaner . /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( . /opt/conda/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( . Right now, cleaner object has all the images in Cricket class from the Train data. So cleaner.fns = List of all file paths shown above. and cleaner.delete() and cleaner.change() return all the indices in cleaner.fns which we marked to delete or which we changes the classes. . print(cleaner.delete()) #list of indexes(for cleaner.fns) which were deleted from above print(cleaner.change()) #list of (image indexes from cleaner.fns, correct class) which were changed from one class to another print(cleaner.fns) . (#0) [] (#0) [] (#30) [Path(&#39;balls/baseball/00000119.png&#39;),Path(&#39;balls/baseball/00000138.jpg&#39;),Path(&#39;balls/baseball/00000076.gif&#39;),Path(&#39;balls/baseball/00000101.png&#39;),Path(&#39;balls/baseball/00000066.png&#39;),Path(&#39;balls/baseball/00000030.png&#39;),Path(&#39;balls/baseball/00000065.jpg&#39;),Path(&#39;balls/baseball/00000094.jpg&#39;),Path(&#39;balls/baseball/00000087.jpg&#39;),Path(&#39;balls/baseball/00000099.jpg&#39;)...] . for idx in cleaner.delete(): cleaner.fns[idx].unlink() #delete files which were marked above for idx,cat in cleaner.change(): #cleaner.change() is a list of tuples shutil.move(str(cleaner.fns[idx]), path/cat) # change paths to correct class . Exporting Model to a .pkl File . learn.export() # exporting model in learn object as a pkl file . path1 = Path() # path to the current folder. Path() is used to define a path path1 . Path(&#39;.&#39;) . path1.ls(file_exts = &#39;.pkl&#39;) # list all files with .pkl extension . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path1/&#39;export.pkl&#39;) # load weights . learn_inf.predict(&#39;images/sachin_signed_ball.jpg&#39;) # this image is and out of sample image of cricket #ball signed by sachin. Our Model works!! . (&#39;cricket&#39;, tensor(1), tensor([0.0196, 0.8899, 0.0905])) . (&#39;cricket&#39;, tensor(1), tensor([6.0704e-04, 9.7703e-01, 2.2365e-02])) . The class that our model predicts | That class&#39;s index in learn_inf.dls.vocab | probability/confidence for each class in the order as given in learn_inf.dls.vocab | the inference part of our model(learn_inf) already knows the class names because dataloader is also loaded with our weights in load_learner object. Hence we have the Vocab and the class names. dataloader can be called from load_learner() object. . print(learn_inf.dls) print(learn_inf.dls.vocab) . &lt;fastai.data.core.DataLoaders object at 0x7efcac05b9d0&gt; (#3) [&#39;baseball&#39;,&#39;cricket&#39;,&#39;soccer&#39;] . Creating a Notebook App . Widgets: FileUpload . btn_upload = widgets.FileUpload( accept=&#39;&#39;, # Accepted file extension e.g. &#39;.txt&#39;, &#39;.pdf&#39;, &#39;image/*&#39;, &#39;image/*,.pdf&#39; multiple=False) # True to accept multiple files upload else False btn_upload #placeholder for uploader button . Widgets: Output . img = PILImage.create(btn_upload.data[-1]) # store image that&#39;s uploaded img . out_pl = widgets.Output() out_pl.clear_output() out_pl #Shows no output yet. Once out_pl is filled, then shows value at placeholder . with out_pl: display(img.to_thumb(128,128)) #output is shown from your end, so this code #display is quite a handy function, you can display youtube videos too . Widgets:label, used to show the prediction result . pred,pred_idx,probs = learn_inf.predict(img) lbl_pred = widgets.Label(&#39;Here is the ball&#39;) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . Widgets:Run, used to run classifier . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): #function to run on click event on classify button img = PILImage.create(btn_upload.data[-1]) #save image uploaded out_pl.clear_output() #clear the output display with out_pl: display(img.to_thumb(128,128)) # set new output display pred,pred_idx,probs = learn_inf.predict(img) #learn_inf.predict gives 3 values seen above lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; #setting label value btn_run.on_click(on_click_classify) . Bringing all widgets together . btn_upload1 = widgets.FileUpload( accept=&#39;&#39;, # Accepted file extension e.g. &#39;.txt&#39;, &#39;.pdf&#39;, &#39;image/*&#39;, &#39;image/*,.pdf&#39; multiple=False) out_pl1 = widgets.Output() lbl_pred1 = widgets.Label(&#39;Here is the ball&#39;) btn_run1 = widgets.Button(description=&#39;Classify&#39;) btn_run1.on_click(on_click_classify) . This cell below is the essence, here we define what needs to be done in the function, and when is the function triggered. . # 1. store image # 2. clear the previousoutput # 3. show the new output # 4. run the learned inference and get the probability values and prediction # 5. model.predict gives 1.prediction, 2. index of prediction in vocab, 3. probability # of each class in the same order as in vocab. So the 2nd term(index) can be used to find # probability with which the predicted class is supported. def on_click_classify(change): #function to run on click event img = PILImage.create(btn_upload1.data[-1]) #save image uploaded out_pl1.clear_output() #clear the output display with out_pl1: display(img.to_thumb(128,128)) # set new output display pred,pred_idx,probs = learn_inf.predict(img) #learn_inf.predict gives 3 values seen above lbl_pred1.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; #setting label value btn_run1.on_click(on_click_classify) #defining the event and defining the function . VBox([widgets.Label(&#39;Select your ball!&#39;), btn_upload1, btn_run1, out_pl1, lbl_pred1]) .",
            "url": "https://ashwanirajan.github.io/MyFastaiJourney/2020/10/16/L3_Sports_Ball_Classifier_Nb_App.html",
            "relUrl": "/2020/10/16/L3_Sports_Ball_Classifier_Nb_App.html",
            "date": " • Oct 16, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "Starts with a basic introduction of terms. I don&#39;t need to type what classification and regression is at this stage of my career. :D . Learner(data, architecture, metric): it contains labelled data and the model. So learner tried to find what are the best prameters for the model which fit the data and labels gien to it. Metric is a function called out on validation set after each epoch that predicts the quality of prediction of our architecture. Epoch is when the entire training set is covered once. . Loss vs Metric: Metrics normally depend on the accuracy of predictions, i.e. as long as 640/1000 examples are classified correctly, the metric would remain same. Metrics are generally not very sensitive of the small changes in weights/parameters, e.g. Error rate, accuracy etc. On the other hand, loss changes even on changing the parameters by very small extent. . Metric is the ting we care about, loss is what the computer is using to update the parameters to improve the model performance. . Overfitting: Memorizing a particultar dataset on which model is trained and not learning about the classification task for which the model is trained, i.e. identifying cat/dog breeds in this case. . learn.fine_tune(): used for transfer learning. Weights of pre-trained models are updated using a different dataset(our dataset) that what the model was originally trained on for additional epochs. one epoch to just fit the parts of model which have been added to the model for this particular dataset, then as many additional epochs as required to fit the entire model, especially the earlier layers which need to change slower and have been observed that they don&#39;t change much. . Catastrophic Forgetting A model pe-trained on image-net would forget the information it learned from imagenet once it is fine tuned for our purpose. To keep it good at both tasks, we need to put examples from previous tasks as well. . Visualising Alexnet: layer by layer visualisation of CNN filters and the part of images which activate those filters. As we move deeper, we see that the model is able to identify more and more sophisticated elements. . CNNs are used to identify other things as well, which can be converted to pictures. E.g. sound by plotting their frequency over time, fraud detection by using mouse clicks and tracings etc. . Find Different state of the art pre-trained models for transfer learning at ModelZoo . Deep learning not always good choice in tabular data but works well at data involving high cardinal variables like zip code, product id etc. . The Practice of Deep Learning . Vision: Detection, Classification | Text: Classification, Conversation(pretty bad at it) | Tabular: High Cardinality | RecSys: Predication &lt;&gt; Recommendation | Multimodal: different types of data, eg: putting captions on photos, | Other: ULNfit(for NLP) is good at protein analysis. | Recommendations vs Predictions: What Amazon does is that it shows you the prediction result of their collaborative filtering model, ie if someone has bought a bok from an author, Amazon tries to advertise the same author&#39;s books instead of the same genre/type of book that i am interested in. If I am interested in an author&#39;s book,I would probably know about the other books of the same author, an better way to recommend would be to give you books of same genre/kind. . Some Stuff about P-values and the practical implementation of a Deep learning Model . Null Hypothesis Significance Testing Never Worked - Prof. Frank Harrell, Vanderbilt University, Prof. of Biostatistics. . P-values also disapproved by American Statistical Association: they released a press note of 7 points about p-values: It then provides six principles to guide p-value usage: . https://www.crossfit.com/health/the-asa-statement-on-p-values-context-process-and-purpose#:~:text=The%20ASA%20defines%20the%20p,extreme%20than%20its%20observed%20value. . P-values indicate how incompatible the data are with a specified statistical model. In the case of “null hypothesis” testing, a lower p-value indicates the belief that there is no difference between two tested groups (e.g., a control and a treatment group). The lower the p-value, the less compatible the data are with the null hypothesis. | P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. The p-value merely tells us how compatible a particular explanation of the data (i.e., that there is no difference between groups) is with the data itself. It does not tell us how likely this explanation, or its converse, is to be true. | Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold. The design of the study, the quality of the measurements, and other data surrounding the studied phenomenon should all inform how a p-value is interpreted. A single p-value cannot denote the truth or falsehood of a statement or hypothesis. The widespread use of “statistical significance” (generally interpreted as “p 0.05”) as a license for making a claim of a scientific finding (or implied truth) leads to considerable distortion of the scientific process. | Proper inference requires full reporting and transparency. Selectively reporting p-values, calculating p-values only for some hypotheses, or discussing only data where the calculated p-value was below a specific threshold renders the data distorted and uninterpretable. The p-value of a particular analysis can only be assessed with complete transparency regarding other analyses performed and their results. | A p-value, or statistical significance, does not measure the size of an effect or the importance of a result. A very low p-value may indicate a small or unimportant event measured very precisely, while a large or important effect may generate a large p-value if measurement precision is low. The p-value tells us nothing about the size of an effect or its scientific, human, or economic significance. | By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis. A low p-value provides only weak evidence against a null hypothesis, while a high p-value does not imply the truth of the null. Other hypotheses may be more consistent with the observed data. Any p-value should be the start, not end, of an analytic approach. | Covid-19 paper . The covid-19 paper has done multiple linear regression and listed the variables which show a relationship, i.e. a p-value &lt; 0.01 with three stars. . Important thing to note: if p-value &gt; threshold(0.01/0.05/0.1 etc) then it does not mean that no relationship b/w two variables exist(or the null hypothesis is true/such a distribution is random). There could still be a relationship, only that there could be a few more dependent variables playing a role and leading to a random distribution when we look at the relationship b/w two variables. . Another example is when we don&#39;t have enough data. Suppose there are only 10 data points, we might not get a p-value &lt;0.01. That doesn&#39;t mean there is no relationship. The way to make sure is that, we try the exact opposite null hypothesis and check if we can reject that null hypothesis. If not, then it means that we just don&#39;t have enough data to conclude anything. . The paper actually does a multivariate regression model with temperature, humidity, population density and GDP, but the graph only shows a univariate relationship. So the variation in R vs Temp plot is not randon, there are a lot of other control variables like Pop. density and others mentioned above which result in these variations. . Jeremy says, p-value is not a mesure of the practical importance, instead the slope is. The Equation R = A - BT - CRel. Humidity, is obtained when control variables are omitted and this suggests that if this model is correct it shows that 2 cities which are exactly same in terms of other variables but have different T and Humidity might have very different spread. . Starting Your Project . Designing Great Data products: https://www.oreilly.com/radar/drivetrain-approach-data-products/ . Need to revisit: All 4 possibilities and their consequences. The concept of prior. . Gathering Data - Bing API Search . To download images with Bing Image Search, sign up at Microsoft for a free account. You will be given a key, which you can copy and enter in a cell as follows (replacing &#39;XXX&#39; with your key and executing it): . key = &#39;27109ffc3c0649ef853678985f164701&#39; . got the key here: https://azure.microsoft.com/en-us/try/cognitive-services/my-apis/?apiSlug=search-api-v7 . from azure.cognitiveservices.search.imagesearch import ImageSearchClient as api from msrest.authentication import CognitiveServicesCredentials as auth . I had made an azure account with a previous microsoft account(from IITG) that I had. It was quite different from what is mentioned in the forum post mentioned by Jeremy. I had three options while making the account, 1. get a 7 day free trial, 2. Get free Azure account and 3. Get paid subscription. I went for 2. get free azure account. It was repeatedly mentioned that I won&#39;t be charged until I upgrade, but when I went to the API page, I see that I have only a 7 day free trial available. I had to give in my Card details though. . def search_images_bing(key, term, min_sz = 128): client = api(&#39;https://api.cognitive.microsoft.com&#39;, auth(key)) return L(client.images.search(query=term,count = 150, min_height = min_sz, min_width = min_sz).value) . L is a class in fastbook. It is fastai&#39;s alternatiive for python lists. . search_images_bing . &lt;function __main__.search_images_bing(key, term, min_sz=128)&gt; . api actually returns 150 items. There are not links. Each item is a dictionary with some keys. One key is &#39;content_url&#39; which is the only thing required for us now. So the L class has an attribute &#39;attrgot&#39; which can be used to get the value of a particular key name from all idems, This is done in the next cell. . results = search_images_bing(key, &#39;soccer ball&#39;) ims = results.attrgot(&#39;content_url&#39;) len(ims) . 150 . Looking at Some Examples . # print(i) . So now ims has a list of URLs for soccer ball images. You can download content from a URL at a particular destination like given in the below slide. . dest = &#39;images/soccerball1.jpg&#39; download_url(ims[0], dest) . Image.open() is from PIL library, which does not need to be called separately because it is one of the dependencies which is called when you call &#39;From fastbook import *&#39; . im = Image.open(dest) im.to_thumb(128,128) . Creating Data Folders . ball_types = &#39;cricket&#39;,&#39;soccer&#39;,&#39;baseball&#39; path = Path(&#39;balls&#39;) ## creates a path in current folder . if not path.exists(): path.mkdir() for o in ball_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} ball&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . fns = get_image_files(path) ## geting the folder path for all images in balls folder. ## Here all the image paths in all three folders are listed down. fns . (#443) [Path(&#39;balls/cricket/00000007.jpg&#39;),Path(&#39;balls/cricket/00000006.jpg&#39;),Path(&#39;balls/cricket/00000004.jpg&#39;),Path(&#39;balls/cricket/00000003.jpg&#39;),Path(&#39;balls/cricket/00000008.jpg&#39;),Path(&#39;balls/cricket/00000002.jpg&#39;),Path(&#39;balls/cricket/00000005.jpg&#39;),Path(&#39;balls/cricket/00000000.png&#39;),Path(&#39;balls/cricket/00000001.jpg&#39;),Path(&#39;balls/cricket/00000009.png&#39;)...] . Removing non-image paths . failed = verify_images(fns) ## checks whether that path contains image or not. failed . (#0) [] . path.unlink deletes files. https://stackoverflow.com/questions/42636018/python-difference-between-os-remove-and-os-unlink-and-which-one-to-use/42636082 . .map() calls the function inside the beackets on each element of the object it is attributed to. It is a part of L class. Here, map() calls path.unlink() function on each element of list L=failed. . failed.map(Path.unlink); . From Data to DataLoaders . ImageDataLoaders is a factory method. We have a more flexible way called DataBlock. . balls = DataBlock( blocks=(ImageBlock, CategoryBlock), #independent and dependent variable get_items=get_image_files, # get list of all filenames needed, See 3rd cell in create data folders section to know about function splitter=RandomSplitter(valid_pct=0.2, seed=42), #seed to fix the validation set everytime we run it get_y=parent_label, # how do we label the data.(By the name of parent folder, a pre-defined way in pytorch) item_tfms=Resize(128)) #resize images . The workflow of DataBlock: . get_items is called first. This would list out paths to our entire data. | get_x, get_y is called next. This would define the data and labels | blocks[0], blocks[1] is called. Need to read more on what these mean. | item_tfms are called to include all the required transformations of independent var. &gt; splitter is called which splits the data into training and validation . | A dataloader is called. Dataloader takes a batch(default 64) images so that this batch is run together on the GPU. . | batch_tfms: later | Datablocks return a dataloader a training and validation dataloaders. Dataloaders are a set of 64 images stacked together for one GPU run. . dls = balls.dataloaders(path) . path # the folder with data is given to dataloaders as input . Path(&#39;balls&#39;) . dls.valid.show_batch(max_n=4, nrows=1) # to see a batch from validation set . Training Your Model . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.218184 | 0.134097 | 0.034091 | 00:06 | . epoch train_loss valid_loss error_rate time . 0 | 0.313904 | 0.057729 | 0.011364 | 00:06 | . 1 | 0.240513 | 0.051974 | 0.011364 | 00:06 | . 2 | 0.175671 | 0.063505 | 0.011364 | 00:06 | . 3 | 0.138641 | 0.065315 | 0.011364 | 00:06 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(4, nrows=1) . Exporting Model to a .pkl File . learn.export() # exporting model in learn object as a pkl file . path1 = Path() # path to the current folder. Path() is used to define a path path1 . Path(&#39;.&#39;) . path1.ls(file_exts = &#39;.pkl&#39;) # list all files with .pkl extension . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path1/&#39;export.pkl&#39;) # load weights . learn_inf.predict(&#39;images/sachin_signed_ball.jpg&#39;) # this image is and out of sample image of cricket #ball signed by sachin. Our Model works!! . (&#39;cricket&#39;, tensor(1), tensor([0.0194, 0.7058, 0.2748])) .",
            "url": "https://ashwanirajan.github.io/MyFastaiJourney/2020/10/16/L2_The_Sports_Ball_Classifier.html",
            "relUrl": "/2020/10/16/L2_The_Sports_Ball_Classifier.html",
            "date": " • Oct 16, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ashwanirajan.github.io/MyFastaiJourney/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ashwanirajan.github.io/MyFastaiJourney/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}